{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e042c093-a504-4e71-aff1-dc5b8d98cec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T18:19:53.411948Z",
     "start_time": "2023-09-22T18:19:47.896139500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import json\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import data as tf_data\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, log_loss\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Union\n",
    "\n",
    "from utils import print_progress_bar\n",
    "\n",
    "print(\"Modules imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "296ecdfb-56f0-4abf-b6f3-0e53210c3668",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T16:36:04.029309900Z",
     "start_time": "2023-09-23T16:36:04.010353100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:\n",
      "/physical_device:CPU:0\n",
      "/physical_device:GPU:0\n",
      "The model will run on GPU: /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# variables\n",
    "all_labels = ['nature', 'country', 'city']\n",
    "path = \"../datasets/all_data/entropy_results.json\"\n",
    "parallel_jobs = 5\n",
    "\n",
    "# hyperparameters\n",
    "test_part = 0.1\n",
    "epochs = 20\n",
    "batch_size = 256\n",
    "learning_rate = 0.005\n",
    "\n",
    "# check gpu\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"Available devices:\")\n",
    "for device in devices:\n",
    "    print(device.name)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(f\"The model will run on GPU: {physical_devices[0].name}\")\n",
    "else:\n",
    "    print(\"No GPU found, the model will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ac6c39-f8b1-411d-a3be-533d511975a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2023-09-20T15:44:33.344049800Z",
     "start_time": "2023-09-20T15:44:33.307149900Z"
    }
   },
   "outputs": [],
   "source": [
    "# heads\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "                self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(self.head_dim)\n",
    "        self.wk = tf.keras.layers.Dense(self.head_dim)\n",
    "        self.wv = tf.keras.layers.Dense(self.head_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Q = self.wq(inputs)\n",
    "        K = self.wk(inputs)\n",
    "        V = self.wv(inputs)\n",
    "\n",
    "        matmul_qk = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        depth = tf.cast(tf.shape(K)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "        output = tf.matmul(attention_weights, V)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b13227e-69a6-4f98-9882-1b3711c07a6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T18:30:29.657985900Z",
     "start_time": "2023-09-23T18:30:29.625079Z"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "class EntropyClassifier(tf.keras.Model):\n",
    "    def __init__(self, possible_labels, folder, debug=False):\n",
    "        super(EntropyClassifier, self).__init__()\n",
    "        dwt_output_size = 10\n",
    "        lvl0_output_size = 17\n",
    "        lvl1_output_size = 17\n",
    "        lvl2_output_size = 17\n",
    "        lvl3_output_size = 153\n",
    "\n",
    "        embed_size = dwt_output_size + lvl0_output_size + lvl1_output_size + lvl2_output_size + lvl3_output_size  # = 214\n",
    "\n",
    "        heads = 1  # Choose based on your specific requirements or experimentation\n",
    "        assert embed_size % heads == 0, \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.possible_labels = possible_labels\n",
    "        self.debug = debug\n",
    "        self.folder = folder\n",
    "\n",
    "        self.dwt_input_layer = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(10),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dropout(0.5)\n",
    "        ])\n",
    "        self.lvl0_input_layer = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(17),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dropout(0.5)\n",
    "        ])\n",
    "        self.lvl1_input_layers = [tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(1, (2, 2)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dropout(0.5)\n",
    "        ]) for _ in range(17)]\n",
    "        self.lvl2_input_layers = [tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(1, (2, 2)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "            tf.keras.layers.Dropout(0.5)\n",
    "        ]) for _ in range(17)]\n",
    "        self.lvl3_input_layers = [tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(1, (2, 2)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "            tf.keras.layers.Dropout(0.5)\n",
    "        ]) for _ in range(17)]\n",
    "\n",
    "        # Commenting out the original self-attention layer\n",
    "        # self.self_attention = SelfAttention(embed_size, heads)\n",
    "\n",
    "        # Replacing self-attention with dense layers\n",
    "        self.final_nn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(len(possible_labels), activation='softmax')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        lvl0_inputs, lvl1_inputs, lvl2_inputs, lvl3_inputs, dwt_inputs = inputs\n",
    "\n",
    "        batch_size = tf.shape(dwt_inputs)[0]\n",
    "\n",
    "        # Ensure inputs have a batch dimension\n",
    "        if len(lvl1_inputs.shape) == 3:\n",
    "            lvl1_inputs = tf.expand_dims(lvl1_inputs, axis=0)\n",
    "        if len(lvl2_inputs.shape) == 3:\n",
    "            lvl2_inputs = tf.expand_dims(lvl2_inputs, axis=0)\n",
    "        if len(lvl3_inputs.shape) == 3:\n",
    "            lvl3_inputs = tf.expand_dims(lvl3_inputs, axis=0)\n",
    "\n",
    "        dwt_output = self.dwt_input_layer(dwt_inputs)\n",
    "        lvl0_output = self.lvl0_input_layer(lvl0_inputs)\n",
    "\n",
    "        lvl1_output = tf.concat([self.lvl1_input_layers[i](lvl1_inputs[:, :, :, i:i + 1]) for i in range(17)], axis=-1)\n",
    "        lvl2_output = tf.concat([self.lvl2_input_layers[i](lvl2_inputs[:, :, :, i:i + 1]) for i in range(17)], axis=-1)\n",
    "        lvl3_output = tf.concat([self.lvl3_input_layers[i](lvl3_inputs[:, :, :, i:i + 1]) for i in range(17)], axis=-1)\n",
    "\n",
    "        concatenated_output = tf.concat([tf.reshape(dwt_output, [batch_size, -1]),\n",
    "                                         tf.reshape(lvl0_output, [batch_size, -1]),\n",
    "                                         tf.reshape(lvl1_output, [batch_size, -1]),\n",
    "                                         tf.reshape(lvl2_output, [batch_size, -1]),\n",
    "                                         tf.reshape(lvl3_output, [batch_size, -1])], axis=-1)\n",
    "\n",
    "        # Commenting out the original self-attention layer\n",
    "        # attention_output = self.self_attention(concatenated_output)\n",
    "\n",
    "        # Using the combined dense layers\n",
    "        final_output = self.final_nn(concatenated_output)\n",
    "\n",
    "        if self.debug:\n",
    "            print(dwt_output.shape)\n",
    "            print(lvl0_output.shape)\n",
    "            print(lvl1_output.shape)\n",
    "            print(lvl2_output.shape)\n",
    "            print(lvl3_output.shape)\n",
    "            print(concatenated_output.shape)\n",
    "            print(final_output.shape)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "    def train_model(self, dataset, epochs=100, batch_size=64, lr=0.01):\n",
    "        formatted_dataset = format_dataset(dataset)\n",
    "        train_dataset = formatted_dataset.batch(batch_size)\n",
    "\n",
    "        loss = None\n",
    "        t0 = time.time()\n",
    "        n = int(len(dataset) // batch_size) + 1\n",
    "\n",
    "        criterion = CategoricalCrossentropy(from_logits=False)\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=lr,\n",
    "            decay_steps=10000,\n",
    "            decay_rate=0.9)\n",
    "        optimizer = Adam(learning_rate=lr_schedule, clipnorm=1.0)\n",
    "\n",
    "        checkpoint_filepath = os.path.join(self.folder, 'checkpoint.chk')\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            save_weights_only=True,\n",
    "            monitor='loss',\n",
    "            mode='min',\n",
    "            save_best_only=True)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Starting epoch {epoch + 1}/{epochs}...')\n",
    "            t1 = time.time()\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(train_dataset):\n",
    "                target = tf.convert_to_tensor([tf.one_hot(t, len(self.possible_labels)) for t in target],\n",
    "                                              dtype=tf.float32)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    output = self(data, training=True)\n",
    "                    loss = criterion(target, output)\n",
    "                gradients = tape.gradient(loss, self.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "                print_progress_bar('Batches processed', batch_idx + 1, n, start_time=t1)\n",
    "\n",
    "            if loss is not None:\n",
    "                elps = int(time.time() - t0)\n",
    "                elps_m, elps_s = divmod(int(elps), 60)\n",
    "                print(f'\\nEpoch {epoch + 1} completed, Loss: {loss.numpy()}, Time elapsed: {elps_m}:{elps_s}')\n",
    "\n",
    "            # Save the model at the end of each epoch\n",
    "            self.save_weights(checkpoint_filepath)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        if len(inputs[0].shape) != 4 or inputs[0].shape[0] != 1:\n",
    "            raise ValueError(\"Input batch size should be 1\")\n",
    "\n",
    "        output = self(inputs, training=False)\n",
    "        probabilities = tf.nn.softmax(output)\n",
    "        max_index = tf.argmax(probabilities, axis=1)\n",
    "        max_index_int = int(max_index[0].numpy())\n",
    "        label_prob_dict = {label: prob.numpy() for label, prob in zip(self.possible_labels, probabilities[0])}\n",
    "\n",
    "        return str(self.possible_labels[max_index_int]), label_prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d63a42-3854-4365-a41b-87438ab6982e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2023-09-22T18:23:19.270632400Z",
     "start_time": "2023-09-22T18:23:19.249688800Z"
    }
   },
   "outputs": [],
   "source": [
    "# data preprocessing functions\n",
    "def process_entry(entry):\n",
    "    \"\"\"Process the entropy results to extract the levels.\"\"\"\n",
    "    label = entry['label']\n",
    "    machine_input = {0: [], 1: [], 2: [], 3: [], 'dwt': []}\n",
    "    for ent in entry['entropy_results']:\n",
    "\n",
    "        if ent['method'] == 'dwt':\n",
    "            machine_input['dwt'] = tf.convert_to_tensor(ent['result'], dtype=tf.float32)\n",
    "        else:\n",
    "            for lvl, content in enumerate(ent['result']):\n",
    "                machine_input[lvl].append(tf.convert_to_tensor(content, dtype=tf.float32))\n",
    "\n",
    "    machine_input[0] = tf.concat(machine_input[0], axis=-1)\n",
    "    machine_input[1] = tf.concat(machine_input[1], axis=-1)\n",
    "    machine_input[2] = tf.concat(machine_input[2], axis=-1)\n",
    "    machine_input[3] = tf.concat(machine_input[3], axis=-1)\n",
    "    machine_input['dwt'] = tf.reshape(machine_input['dwt'], [1, 1, 10])\n",
    "\n",
    "    return {'input': machine_input, 'label': label}\n",
    "\n",
    "\n",
    "def format_dataset(dataset):\n",
    "    \"\"\"Formats and shuffles the dataset for training\"\"\"\n",
    "    label_num = {'nature': 0, 'country': 1, 'city': 2}\n",
    "    formatted_dataset = []\n",
    "    for item in dataset:\n",
    "        machine_input = item['input']\n",
    "        label = label_num[item['label']]\n",
    "        formatted_dataset.append((\n",
    "            (\n",
    "                machine_input[0],\n",
    "                machine_input[1],\n",
    "                machine_input[2],\n",
    "                machine_input[3],\n",
    "                machine_input['dwt']\n",
    "            ),\n",
    "            label\n",
    "        ))\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        lambda: iter(formatted_dataset),\n",
    "        output_signature=(\n",
    "            (\n",
    "                tf.TensorSpec(shape=(1, 1, 17), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(2, 2, 17), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(4, 4, 17), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(8, 8, 17), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(1, 1, 10), dtype=tf.float32)\n",
    "            ),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "        )\n",
    "    ).shuffle(buffer_size=len(dataset))\n",
    "\n",
    "\n",
    "def process_json(path, test_part, parallel_jobs=4):\n",
    "    \"\"\"Process JSON data to extract dataset and features.\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    dataset = []\n",
    "\n",
    "    t = time.time()\n",
    "    n = len(metadata)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=parallel_jobs) as executor:\n",
    "        futures = [executor.submit(process_entry, entry) for entry in metadata]\n",
    "        for i, future in enumerate(as_completed(futures)):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                dataset.append(result)\n",
    "            print_progress_bar('Processed entry', i + 1, n, t)\n",
    "\n",
    "    if isinstance(test_part, float):\n",
    "        i = int(test_part * len(dataset))\n",
    "    elif isinstance(test_part, str):\n",
    "        i = int(test_part)\n",
    "    else:\n",
    "        raise ValueError(\"Incompatible format for 'test_part'.\")\n",
    "\n",
    "    test_set = dataset[-i:]\n",
    "    dataset = dataset[:-i]\n",
    "\n",
    "    num_classes = len(all_labels)\n",
    "    dataset_length = len(dataset)\n",
    "\n",
    "    return dataset, test_set, num_classes, dataset_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2196569-713f-4310-bb1b-39221c6f2185",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2023-09-22T18:23:23.006412Z",
     "start_time": "2023-09-22T18:23:22.991389100Z"
    }
   },
   "outputs": [],
   "source": [
    "# model evaluation functions\n",
    "def calculate_stats(y_true, y_pred, y_prob):\n",
    "    \"\"\"\n",
    "    This function calculates various statistics like confusion matrix, precision, recall, F1 score and log loss.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (numpy array): Array of true labels\n",
    "    y_pred (numpy array): Array of predicted labels\n",
    "    y_prob (numpy array): Array of predicted probabilities\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing all the calculated statistics\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_true)\n",
    "    y_true_bin = lb.transform(y_true)\n",
    "\n",
    "    y_prob = np.array([list(item.values()) for item in y_prob])\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    logloss = log_loss(y_true_bin, y_prob, labels=lb.classes_)\n",
    "\n",
    "    stats = {\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'log_loss': logloss\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_set):\n",
    "    stats = {'test_samples': 0, 'right_predictions': 0}\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "\n",
    "    for test in test_set:\n",
    "        stats['test_samples'] += 1\n",
    "\n",
    "        machine_input = test['input']\n",
    "        inputs = (\n",
    "            tf.expand_dims(machine_input[0], axis=0),\n",
    "            tf.expand_dims(machine_input[1], axis=0),\n",
    "            tf.expand_dims(machine_input[2], axis=0),\n",
    "            tf.expand_dims(machine_input[3], axis=0),\n",
    "            tf.expand_dims(machine_input['dwt'], axis=0)\n",
    "        )\n",
    "        true_label = test['label']\n",
    "\n",
    "        predicted_label, label_probs = model.predict(inputs)\n",
    "\n",
    "        y_true.append(true_label)\n",
    "        y_pred.append(predicted_label)\n",
    "        y_prob.append(label_probs)\n",
    "\n",
    "        if predicted_label == true_label:\n",
    "            stats['right_predictions'] += 1\n",
    "\n",
    "    stats['success_rate'] = 100 * stats['right_predictions'] / stats['test_samples']\n",
    "\n",
    "    # # Get the calculated stats\n",
    "    # stats.update(calculate_stats(np.array(y_true), np.array(y_pred), np.array(y_prob)))\n",
    "\n",
    "    print(f\"{stats['right_predictions']} samples out of {stats['test_samples']} were predicted correctly.\\n\"\n",
    "          f\"The model's success rate is: {stats['success_rate']}%\")\n",
    "    # print(f\"Confusion Matrix: \\n{stats['confusion_matrix']}\")\n",
    "    # print(f\"Precision: {stats['precision']}\")\n",
    "    # print(f\"Recall: {stats['recall']}\")\n",
    "    # print(f\"F1 Score: {stats['f1_score']}\")\n",
    "    # print(f\"Log Loss: {stats['log_loss']}\")\n",
    "\n",
    "    return stats['success_rate'] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7f62024-063c-4f89-b901-82f421741a94",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-09-22T18:24:02.134091200Z",
     "start_time": "2023-09-22T18:23:24.644384400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed entry: ██████████████████████████████████████████████████ | Completed: 7495/7495, 100.0% | Time elapsed: 0:00:22/0:00:22 | Time left: ~= 0:00:00 |\n",
      "Dataset processed.\n",
      "Total number of entries in the dataset: 6746\n",
      "Total number of entries in the test set: 749\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataset, test_set, num_classes, dataset_length = process_json(path, test_part)\n",
    "print('\\nDataset processed.')\n",
    "print(f\"Total number of entries in the dataset: {dataset_length}\")\n",
    "print(f\"Total number of entries in the test set: {len(test_set)}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d58f1a48-0f23-46a5-be18-3e8533dc1e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T18:31:27.140299800Z",
     "start_time": "2023-09-23T18:31:26.933075200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created\n"
     ]
    }
   ],
   "source": [
    "# model creation\n",
    "model_folder = f\"../models/EntropyClassifier\"\n",
    "model_name = f\"instance_e={epochs}_ds={dataset_length}_bs={batch_size}\"\n",
    "model = EntropyClassifier(all_labels, model_folder)\n",
    "# model.debug = True\n",
    "print('Model created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db52a15-4d6d-479c-95f5-3d5509f9698a",
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-09-23T18:31:29.083830800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/20...\n",
      "Batches processed: ██████████████████████████████████████████████████ | Completed: 27/27, 100.0% | Time elapsed: 0:00:14/0:00:14 | Time left: ~= 0:00:00 |\n",
      "Epoch 1 completed, Loss: 0.7871942520141602, Time elapsed: 0:14\n",
      "Starting epoch 2/20...\n",
      "Batches processed: ██████████████████████████████████████████████████ | Completed: 27/27, 100.0% | Time elapsed: 0:00:11/0:00:11 | Time left: ~= 0:00:00 |\n",
      "Epoch 2 completed, Loss: 0.7537193298339844, Time elapsed: 0:26\n",
      "Starting epoch 3/20...\n",
      "Batches processed: ██████████████████████████████████████████████████ | Completed: 27/27, 100.0% | Time elapsed: 0:00:11/0:00:11 | Time left: ~= 0:00:00 |\n",
      "Epoch 3 completed, Loss: 0.6115423440933228, Time elapsed: 0:38\n",
      "Starting epoch 4/20...\n",
      "Batches processed: ██████████████████████████████████████████████████ | Completed: 27/27, 100.0% | Time elapsed: 0:00:10/0:00:10 | Time left: ~= 0:00:00 |\n",
      "Epoch 4 completed, Loss: 0.7252815961837769, Time elapsed: 0:49\n",
      "Starting epoch 5/20...\n",
      "Batches processed: ██████████████████████████████████████████████████ | Completed: 27/27, 100.0% | Time elapsed: 0:00:10/0:00:10 | Time left: ~= 0:00:00 |\n",
      "Epoch 5 completed, Loss: 0.6145799160003662, Time elapsed: 1:0\n",
      "Starting epoch 6/20...\n",
      "Batches processed: ██████████████████████████████████████████████████ | Completed: 27/27, 100.0% | Time elapsed: 0:00:10/0:00:10 | Time left: ~= 0:00:00 |\n",
      "Epoch 6 completed, Loss: 0.5183320045471191, Time elapsed: 1:11\n",
      "Starting epoch 7/20...\n",
      "Batches processed: ██████████████████████████████████████████████████ | Completed: 27/27, 100.0% | Time elapsed: 0:00:10/0:00:10 | Time left: ~= 0:00:00 |\n",
      "Epoch 7 completed, Loss: 0.5892757773399353, Time elapsed: 1:22\n",
      "Starting epoch 8/20...\n",
      "Batches processed: ██████████████████████████████████████████████████ | Completed: 27/27, 100.0% | Time elapsed: 0:00:10/0:00:10 | Time left: ~= 0:00:00 |\n",
      "Epoch 8 completed, Loss: 0.7324471473693848, Time elapsed: 1:33\n",
      "Starting epoch 9/20...\n",
      "Batches processed: ██████████████████████████████████████████████████ | Completed: 27/27, 100.0% | Time elapsed: 0:00:11/0:00:11 | Time left: ~= 0:00:00 |\n",
      "Epoch 9 completed, Loss: 0.5225928425788879, Time elapsed: 1:45\n",
      "Starting epoch 10/20...\n",
      "Batches processed: ██████████████████████████████████████████████████ | Completed: 27/27, 100.0% | Time elapsed: 0:00:11/0:00:11 | Time left: ~= 0:00:00 |\n",
      "Epoch 10 completed, Loss: 0.792860746383667, Time elapsed: 1:56\n",
      "Starting epoch 11/20...\n",
      "Batches processed: ██████████████████████████████████████████████████ | Completed: 27/27, 100.0% | Time elapsed: 0:00:12/0:00:12 | Time left: ~= 0:00:00 |\n",
      "Epoch 11 completed, Loss: 0.5731614828109741, Time elapsed: 2:8\n",
      "Starting epoch 12/20...\n",
      "Batches processed: ██████████████████████████████████████████████████ | Completed: 27/27, 100.0% | Time elapsed: 0:00:11/0:00:11 | Time left: ~= 0:00:00 |\n",
      "Epoch 12 completed, Loss: 0.6414777040481567, Time elapsed: 2:20\n",
      "Starting epoch 13/20...\n",
      "Batches processed: ██████████████████████████████████████████████████ | Completed: 27/27, 100.0% | Time elapsed: 0:00:11/0:00:11 | Time left: ~= 0:00:00 |\n",
      "Epoch 13 completed, Loss: 0.6350469589233398, Time elapsed: 2:32\n",
      "Starting epoch 14/20...\n",
      "Batches processed: █████████████████████████████████████------------- | Completed: 20/27, 74.1% | Time elapsed: 0:00:09/0:00:12 | Time left: ~= 0:00:03 |"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "# model.debug = False\n",
    "model.train_model(dataset, epochs=epochs, batch_size=batch_size, lr=learning_rate)\n",
    "print('Model trained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0dc9932-29f0-4fbe-b8e2-cd18eb3e548a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T18:42:19.589253100Z",
     "start_time": "2023-09-22T18:42:19.424870900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "if not os.path.exists(model_folder):\n",
    "    os.mkdir(model_folder)\n",
    "model.save_weights(os.path.join(model_folder, model_name))\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3168957a-fdc3-4f50-94c5-795ae4533e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model.save_weights(os.path.join(model_folder, model_name))\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d18773b-e2e5-4c83-bb4a-e031d84fe44f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T18:28:25.167299800Z",
     "start_time": "2023-09-23T18:27:33.075687800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 samples out of 749 were predicted correctly.\n",
      "The model's success rate is: 48.197596795727634%\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "comment = 'new architecture. less layers.'\n",
    "result = evaluate_model(model, test_set)\n",
    "\n",
    "perf_json = f\"{model_folder}/performance.json\"\n",
    "if os.path.exists(perf_json):\n",
    "    with open(perf_json, 'r') as f:\n",
    "        perf = json.load(f)\n",
    "else:\n",
    "    perf = []\n",
    "\n",
    "perf.append({'lr': learning_rate,\n",
    "             'epochs': epochs,\n",
    "             'dataset size': dataset_length,\n",
    "             'testset size': len(test_set),\n",
    "             'test part': test_part,\n",
    "             'batch size': batch_size,\n",
    "             'performance': result,\n",
    "             'comment': comment\n",
    "             })\n",
    "with open(perf_json, 'w') as f:\n",
    "    json.dump(perf, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd5773-529f-46fa-a4dc-4ac8fc266842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
