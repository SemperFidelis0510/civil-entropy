{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e042c093-a504-4e71-aff1-dc5b8d98cec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import data as tf_data\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, log_loss\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Union\n",
    "\n",
    "from utils import print_progress_bar\n",
    "print(\"Modules imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "296ecdfb-56f0-4abf-b6f3-0e53210c3668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:\n",
      "/physical_device:CPU:0\n",
      "/physical_device:GPU:0\n",
      "The model will run on GPU: /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# variables\n",
    "all_labels = ['nature', 'country', 'city']\n",
    "path = \"../datasets/all_data/entropy_results_short.json\"\n",
    "parallel_jobs = 5\n",
    "\n",
    "# hyperparameters\n",
    "test_part = 0.05\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "\n",
    "# check gpu\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"Available devices:\")\n",
    "for device in devices:\n",
    "    print(device.name)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(f\"The model will run on GPU: {physical_devices[0].name}\")\n",
    "else:\n",
    "    print(\"No GPU found, the model will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f848128-99e4-465a-bbaf-22893cb97fe0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# data validation functions\n",
    "def validate_element(element: Dict[str, Any]) -> Union[bool, str]:\n",
    "    \"\"\"Validate the format of an element in the dataset.\"\"\"\n",
    "    try:\n",
    "        # Validate 'image' key\n",
    "        image = element['image']\n",
    "        \n",
    "        # Validate 'dwt' key\n",
    "        dwt = image[0]\n",
    "        if not isinstance(dwt, tf.Tensor) or dwt.shape[1] != 10:\n",
    "            return \"Invalid shape or type for 'dwt'. Expected a tensor with shape: (batch_size, 10)\"\n",
    "        \n",
    "        # Validate 'lvl0' key\n",
    "        lvl0 = image[1]\n",
    "        if not isinstance(lvl0, tf.Tensor) or lvl0.shape[1] != 17:\n",
    "            return \"Invalid shape or type for 'lvl0'. Expected a tensor with shape: (batch_size, 17)\"\n",
    "        \n",
    "        # Validate 'lvl1', 'lvl2', and 'lvl3' keys\n",
    "        for i, shape in zip([2, 3, 4], [(2, 2, 1), (4, 4, 1), (8, 8, 1)]):\n",
    "            for lvl in image[i]:\n",
    "                if not isinstance(lvl, tf.Tensor) or lvl.shape[2:] != shape:\n",
    "                    return f\"Invalid shape or type for 'lvl{i-1}'. Expected a tensor with shape: (batch_size, ..., {shape})\"\n",
    "        \n",
    "        # Validate 'label' key\n",
    "        label = element['label']\n",
    "        if not isinstance(label, tf.Tensor) or label.dtype != tf.string:\n",
    "            return \"Invalid format for 'label'. Expected a tensor of strings.\"\n",
    "        \n",
    "        return True\n",
    "    except (KeyError, ValueError, TypeError) as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "# Function to validate all elements in the dataset\n",
    "def validate_dataset(dataset: List[Dict[str, Any]]) -> bool:\n",
    "    return all(validate_element(element) for element in dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25ac6c39-f8b1-411d-a3be-533d511975a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# heads\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(self.head_dim)\n",
    "        self.wk = tf.keras.layers.Dense(self.head_dim)\n",
    "        self.wv = tf.keras.layers.Dense(self.head_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Q = self.wq(inputs)\n",
    "        K = self.wk(inputs)\n",
    "        V = self.wv(inputs)\n",
    "\n",
    "        matmul_qk = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        depth = tf.cast(tf.shape(K)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "        output = tf.matmul(attention_weights, V)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1b13227e-69a6-4f98-9882-1b3711c07a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class EntropyClassifier(tf.keras.Model):\n",
    "    def __init__(self, possible_labels):\n",
    "        super(EntropyClassifier, self).__init__()\n",
    "\n",
    "        self.possible_labels = possible_labels\n",
    "\n",
    "        self.dwt_input_layer = tf.keras.layers.Dense(10, activation='relu')\n",
    "        self.lvl0_input_layer = tf.keras.layers.Dense(17, activation='relu')\n",
    "        self.lvl1_input_layers = [tf.keras.layers.Conv2D(1, (2, 2), activation='relu') for _ in range(17)]\n",
    "        self.lvl2_input_layers = [tf.keras.layers.Conv2D(1, (2, 2), activation='relu') for _ in range(17)]\n",
    "        self.lvl3_input_layers = [tf.keras.layers.Conv2D(1, (2, 2), activation='relu') for _ in range(17)]\n",
    "\n",
    "        self.self_attention_heads = [SelfAttention(10, 1), SelfAttention(17, 1)] + [SelfAttention(4, 1) for _ in range(51)]\n",
    "\n",
    "        self.combination_layer = tf.keras.layers.Concatenate()\n",
    "\n",
    "        self.fc_layer = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(len(possible_labels), activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Method to forward propagate through the model\n",
    "        Args:\n",
    "        inputs (Tensor): Input tensor\n",
    "    \n",
    "        Returns:\n",
    "        Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        dwt_output, lvl0_output, lvl1_outputs, lvl2_outputs, lvl3_outputs = inputs\n",
    "        \n",
    "        lvl1_outputs = [tf.expand_dims(input, axis=0) for input in lvl1_outputs]\n",
    "        lvl2_outputs = [tf.expand_dims(input, axis=0) for input in lvl2_outputs]\n",
    "        lvl3_outputs = [tf.expand_dims(input, axis=0) for input in lvl3_outputs]\n",
    "        \n",
    "        attention_inputs = [dwt_output, lvl0_output] + [tf.keras.layers.Flatten()(output) for output in (lvl1_outputs + lvl2_outputs + lvl3_outputs)]\n",
    "        \n",
    "        attention_outputs = [layer(input) for layer, input in zip(self.self_attention_heads, attention_inputs)]\n",
    "        \n",
    "        # Calculate the common dimension size for concatenation\n",
    "        common_dimension_size = min([tf.shape(output)[-1] for output in attention_outputs])\n",
    "        \n",
    "        # Reshape tensors to have the same size in the last dimension\n",
    "        attention_outputs = [tf.slice(output, [0,0], [-1, common_dimension_size]) for output in attention_outputs]\n",
    "        \n",
    "        combined_output = tf.concat(attention_outputs, axis=-1)\n",
    "        \n",
    "        fc_output = self.fc_layer(combined_output)\n",
    "        output = self.output_layer(fc_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "    def train_model(self, dataset, epochs=100, batch_size=64, lr=0.01):\n",
    "        loss = None\n",
    "        formatted_dataset = format_dataset(dataset)\n",
    "\n",
    "        train_dataset = formatted_dataset.batch(batch_size)\n",
    "\n",
    "        criterion = CategoricalCrossentropy(from_logits=True)\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Starting epoch {epoch+1}/{epochs}')\n",
    "            for batch_idx, (data, target) in enumerate(train_dataset):\n",
    "                target = tf.convert_to_tensor([tf.one_hot(t, len(self.possible_labels)) for t in target], dtype=tf.float32)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    output = self(data, training=True)\n",
    "                    print(f\"Output shape: {output.shape}, Target shape: {target.shape}\")  # Added this line to debug\n",
    "                    loss = criterion(target, output)\n",
    "                gradients = tape.gradient(loss, self.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f'Batch {batch_idx}, Loss: {loss.numpy()}')\n",
    "\n",
    "            if loss is not None:\n",
    "                print(f'Epoch {epoch+1} completed, Loss: {loss.numpy()}')\n",
    "\n",
    "    def predict(self, image):\n",
    "        image = (image[0], image[1], image[2], image[3], image[4])\n",
    "        output_ = self(image, training=False)\n",
    "        probabilities = tf.nn.softmax(output_)\n",
    "        max_index = tf.argmax(probabilities)\n",
    "        label_prob_dict = {label: prob.numpy() for label, prob in zip(self.possible_labels, probabilities[0])}\n",
    "\n",
    "        return str(self.possible_labels[max_index.numpy()]), label_prob_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d9d63a42-3854-4365-a41b-87438ab6982e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def process_entry(entry):\n",
    "    \"\"\"Process the entropy results to extract the levels.\"\"\"\n",
    "    label = entry['label']\n",
    "    machine_input = {0: [], 1: [], 2: [], 3: [], 'dwt': []}\n",
    "    for ent in entry['entropy_results']:\n",
    "    \n",
    "        if ent['method'] == 'dwt':\n",
    "            machine_input['dwt'] = tf.convert_to_tensor(ent['result'], dtype=tf.float32)\n",
    "        else:\n",
    "            for lvl, content in enumerate(ent['result']):\n",
    "                machine_input[lvl].append(tf.convert_to_tensor(content, dtype=tf.float32))\n",
    "    \n",
    "    machine_input[0] = tf.concat(machine_input[0], axis=-1)\n",
    "    machine_input[1] = tf.concat(machine_input[1], axis=-1)\n",
    "    machine_input[2] = tf.concat(machine_input[2], axis=-1)\n",
    "    machine_input[3] = tf.concat(machine_input[3], axis=-1)\n",
    "    machine_input['dwt'] = tf.reshape(machine_input['dwt'], [1, 1, 10])\n",
    "    \n",
    "    return {'input': machine_input, 'label': label}  \n",
    "\n",
    "\n",
    "def format_dataset(dataset):\n",
    "    \"\"\"Formats and shuffles the dataset for training\"\"\"\n",
    "    label_num = {'nature': 0, 'country': 1, 'city': 2}\n",
    "    formatted_dataset = []\n",
    "    for item in dataset:\n",
    "        machine_input = item['input']\n",
    "        label = label_num[item['label']]\n",
    "        formatted_dataset.append((\n",
    "            (\n",
    "                machine_input[0], \n",
    "                machine_input[1], \n",
    "                machine_input[2], \n",
    "                machine_input[3], \n",
    "                machine_input['dwt']\n",
    "            ), \n",
    "            label\n",
    "        ))\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        lambda: iter(formatted_dataset), \n",
    "        output_signature=(\n",
    "            (\n",
    "                tf.TensorSpec(shape=(1, 1, 17), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(2, 2, 17), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(4, 4, 17), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(8, 8, 17), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(1, 1, 10), dtype=tf.float32)\n",
    "            ),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "        )\n",
    "    ).shuffle(buffer_size=len(dataset))\n",
    "\n",
    "\n",
    "def process_json(path, test_part, parallel_jobs=4):\n",
    "    \"\"\"Process JSON data to extract dataset and features.\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    dataset = []\n",
    "\n",
    "    t = time.time()\n",
    "    n = len(metadata)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=parallel_jobs) as executor:\n",
    "        futures = [executor.submit(process_entry, entry) for entry in metadata]\n",
    "        for i, future in enumerate(as_completed(futures)):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                dataset.append(result)\n",
    "            print_progress_bar('Processed entry', i+1, n, t)\n",
    "\n",
    "    if isinstance(test_part, float):\n",
    "        i = int(test_part * len(dataset))\n",
    "    elif isinstance(test_part, str):\n",
    "        i = int(test_part)\n",
    "    else:\n",
    "        raise ValueError(\"Incompatible format for 'test_part'.\")\n",
    "\n",
    "    test_set = dataset[-i:]\n",
    "    dataset = dataset[:-i]\n",
    "\n",
    "    num_classes = len(all_labels)\n",
    "    dataset_length = len(dataset)\n",
    "\n",
    "    return dataset, test_set, num_classes, dataset_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2196569-713f-4310-bb1b-39221c6f2185",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# model evaluation functions\n",
    "def evaluate_model(model, test_set):\n",
    "    stats = {'test_samples': 0, 'right_predictions': 0}\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "    \n",
    "    for test in test_set:\n",
    "        stats['test_samples'] += 1\n",
    "        \n",
    "        image = test['image']\n",
    "        if device == \"GPU\":\n",
    "            image = tf.convert_to_tensor(image)\n",
    "        \n",
    "        predicted_label, label_probs = model.predict(image)\n",
    "        \n",
    "        y_true.append(test[\"label\"])\n",
    "        y_pred.append(predicted_label)\n",
    "        y_prob.append(label_probs[test[\"label\"][0]])\n",
    "        \n",
    "        if predicted_label == test[\"label\"]:\n",
    "            stats['right_predictions'] += 1\n",
    "            print(f'Predicted label: {predicted_label}.  Real label: {test[\"label\"]}. Prediction correct!')\n",
    "        else:\n",
    "            print(f'Predicted label: {predicted_label}.  Real label: {test[\"label\"]}. False prediction.')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    logloss = log_loss(y_true, np.array(y_prob))\n",
    "    \n",
    "    stats['success_rate'] = 100 * stats['right_predictions'] / stats['test_samples']\n",
    "    stats['confusion_matrix'] = conf_matrix\n",
    "    stats['precision'] = precision\n",
    "    stats['recall'] = recall\n",
    "    stats['log_loss'] = logloss\n",
    "    \n",
    "    print(f\"{stats['right_predictions']} samples out of {stats['test_samples']} were predicted correctly.\\n\"\n",
    "          f\"The model's success rate is: {stats['success_rate']}%\")\n",
    "    print(f\"Confusion Matrix: \\n{conf_matrix}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Log Loss: {logloss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f7f62024-063c-4f89-b901-82f421741a94",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed entry: ██████████████████████████████████████████████████ | Completed: 256/256 100.0% | Time elapsed: 00:01/00:01 | Time left: ~00:00Dataset processed.\n",
      "Total number of entries in the dataset: 244\n",
      "Total number of entries in the test set: 12\n",
      "Number of classes: 3\n",
      "Dataset length: 244\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataset, test_set, num_classes, dataset_length = process_json(path, test_part)\n",
    "print('Dataset processed.')\n",
    "print(f\"Total number of entries in the dataset: {dataset_length}\")\n",
    "print(f\"Total number of entries in the test set: {len(test_set)}\")    \n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Dataset length: {dataset_length}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "117ced19-69b5-4697-aa52-5db94b589f9f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "ValueError: `generator` yielded an element of shape (17, 2, 2, 1) where an element of shape (2, 2, 1) was expected.\nTraceback (most recent call last):\n\n  File \"C:\\ProgramData\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"C:\\ProgramData\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\ProgramData\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1005, in generator_py_func\n    raise ValueError(\n\nValueError: `generator` yielded an element of shape (17, 2, 2, 1) where an element of shape (2, 2, 1) was expected.\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# data validity check\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# %whos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mvalidate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print(type(dataset[0]))\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(type(dataset[0]['image']))\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "Cell \u001b[1;32mIn[86], line 37\u001b[0m, in \u001b[0;36mvalidate_dataset\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_dataset\u001b[39m(dataset: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidate_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[86], line 37\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_dataset\u001b[39m(dataset: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mall\u001b[39m(validate_element(element) \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m dataset)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:761\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    760\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    762\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:744\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 744\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    749\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:2727\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2725\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   2726\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2727\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2728\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   2729\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6897\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6895\u001b[0m message \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6896\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 6897\u001b[0m \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_status_to_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: ValueError: `generator` yielded an element of shape (17, 2, 2, 1) where an element of shape (2, 2, 1) was expected.\nTraceback (most recent call last):\n\n  File \"C:\\ProgramData\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"C:\\ProgramData\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\ProgramData\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1005, in generator_py_func\n    raise ValueError(\n\nValueError: `generator` yielded an element of shape (17, 2, 2, 1) where an element of shape (2, 2, 1) was expected.\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "# data validity check\n",
    "# %whos\n",
    "validate_dataset(dataset)\n",
    "# print(type(dataset[0]))\n",
    "# print(type(dataset[0]['image']))\n",
    "for key in dataset[0]['image']:\n",
    "    print(type(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d58f1a48-0f23-46a5-be18-3e8533dc1e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created\n"
     ]
    }
   ],
   "source": [
    "# model creation\n",
    "file_name = f\"EntropyClassifier_e={epochs}_ds={dataset_length}.pth\"\n",
    "model = EntropyClassifier(all_labels)\n",
    "print('Model created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5db52a15-4d6d-479c-95f5-3d5509f9698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/2\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected begin and size arguments to be 1-D tensors of size 4, but got shapes [2] and [2] instead. [Op:Slice]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[177], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel trained.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[175], line 69\u001b[0m, in \u001b[0;36mEntropyClassifier.train_model\u001b[1;34m(self, dataset, epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m     66\u001b[0m target \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor([tf\u001b[38;5;241m.\u001b[39mone_hot(t, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpossible_labels)) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m target], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 69\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Target shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Added this line to debug\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(target, output)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1030\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1026\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1030\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1033\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "Cell \u001b[1;32mIn[175], line 44\u001b[0m, in \u001b[0;36mEntropyClassifier.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     41\u001b[0m common_dimension_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m([tf\u001b[38;5;241m.\u001b[39mshape(output)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m attention_outputs])\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Reshape tensors to have the same size in the last dimension\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mslice(output, [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, common_dimension_size]) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m attention_outputs]\n\u001b[0;32m     46\u001b[0m combined_output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(attention_outputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     48\u001b[0m fc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_layer(combined_output)\n",
      "Cell \u001b[1;32mIn[175], line 44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     41\u001b[0m common_dimension_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m([tf\u001b[38;5;241m.\u001b[39mshape(output)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m attention_outputs])\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Reshape tensors to have the same size in the last dimension\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m [\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_dimension_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m attention_outputs]\n\u001b[0;32m     46\u001b[0m combined_output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(attention_outputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     48\u001b[0m fc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_layer(combined_output)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m    208\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    209\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1107\u001b[0m, in \u001b[0;36mslice\u001b[1;34m(input_, begin, size, name)\u001b[0m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mslice\u001b[39m(input_, begin, size, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1058\u001b[0m   \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Extracts a slice from a tensor.\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \n\u001b[0;32m   1061\u001b[0m \u001b[38;5;124;03m  See also `tf.strided_slice`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;124;03m    A `Tensor` the same type as `input_`.\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1107\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:9404\u001b[0m, in \u001b[0;36m_slice\u001b[1;34m(input, begin, size, name)\u001b[0m\n\u001b[0;32m   9402\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   9403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 9404\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_slice_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9405\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   9406\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[0;32m   9407\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:9430\u001b[0m, in \u001b[0;36m_slice_eager_fallback\u001b[1;34m(input, begin, size, name, ctx)\u001b[0m\n\u001b[0;32m   9428\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28minput\u001b[39m, begin, size]\n\u001b[0;32m   9429\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_Index)\n\u001b[1;32m-> 9430\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSlice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9431\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   9432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n\u001b[0;32m   9433\u001b[0m   _execute\u001b[38;5;241m.\u001b[39mrecord_gradient(\n\u001b[0;32m   9434\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSlice\u001b[39m\u001b[38;5;124m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Expected begin and size arguments to be 1-D tensors of size 4, but got shapes [2] and [2] instead. [Op:Slice]"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.train_model(dataset, epochs=epochs, batch_size=batch_size, lr=learning_rate)\n",
    "print('Model trained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dc9932-29f0-4fbe-b8e2-cd18eb3e548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168957a-fdc3-4f50-94c5-795ae4533e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model.load_state_dict(torch.load(file_name, map_location=device))\n",
    "model.eval()\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d18773b-e2e5-4c83-bb4a-e031d84fe44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "evaluate_model(model, test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
