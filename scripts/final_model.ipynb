{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e042c093-a504-4e71-aff1-dc5b8d98cec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import data as tf_data\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, log_loss\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Union\n",
    "\n",
    "from utils import print_progress_bar\n",
    "print(\"Modules imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "296ecdfb-56f0-4abf-b6f3-0e53210c3668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:\n",
      "/physical_device:CPU:0\n",
      "No GPU found, the model will run on CPU.\n"
     ]
    }
   ],
   "source": [
    "# variables\n",
    "all_labels = ['nature', 'country', 'city']\n",
    "path = \"../datasets/all_data/entropy_results_short.json\"\n",
    "parallel_jobs = 5\n",
    "\n",
    "# hyperparameters\n",
    "test_part = 0.05\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "\n",
    "# check gpu\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"Available devices:\")\n",
    "for device in devices:\n",
    "    print(device.name)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(f\"The model will run on GPU: {physical_devices[0].name}\")\n",
    "else:\n",
    "    print(\"No GPU found, the model will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f848128-99e4-465a-bbaf-22893cb97fe0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# data validation functions\n",
    "def validate_element(element: Dict[str, Any]) -> Union[bool, str]:\n",
    "    \"\"\"Validate the format of an element in the dataset.\"\"\"\n",
    "    try:\n",
    "        # Validate 'image' key\n",
    "        image = element['image']\n",
    "        \n",
    "        # Validate 'dwt' key\n",
    "        dwt = image[0]\n",
    "        if not isinstance(dwt, tf.Tensor) or dwt.shape[1] != 10:\n",
    "            return \"Invalid shape or type for 'dwt'. Expected a tensor with shape: (batch_size, 10)\"\n",
    "        \n",
    "        # Validate 'lvl0' key\n",
    "        lvl0 = image[1]\n",
    "        if not isinstance(lvl0, tf.Tensor) or lvl0.shape[1] != 17:\n",
    "            return \"Invalid shape or type for 'lvl0'. Expected a tensor with shape: (batch_size, 17)\"\n",
    "        \n",
    "        # Validate 'lvl1', 'lvl2', and 'lvl3' keys\n",
    "        for i, shape in zip([2, 3, 4], [(2, 2, 1), (4, 4, 1), (8, 8, 1)]):\n",
    "            for lvl in image[i]:\n",
    "                if not isinstance(lvl, tf.Tensor) or lvl.shape[2:] != shape:\n",
    "                    return f\"Invalid shape or type for 'lvl{i-1}'. Expected a tensor with shape: (batch_size, ..., {shape})\"\n",
    "        \n",
    "        # Validate 'label' key\n",
    "        label = element['label']\n",
    "        if not isinstance(label, tf.Tensor) or label.dtype != tf.string:\n",
    "            return \"Invalid format for 'label'. Expected a tensor of strings.\"\n",
    "        \n",
    "        return True\n",
    "    except (KeyError, ValueError, TypeError) as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "# Function to validate all elements in the dataset\n",
    "def validate_dataset(dataset: List[Dict[str, Any]]) -> bool:\n",
    "    return all(validate_element(element) for element in dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25ac6c39-f8b1-411d-a3be-533d511975a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heads\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(self.head_dim)\n",
    "        self.wk = tf.keras.layers.Dense(self.head_dim)\n",
    "        self.wv = tf.keras.layers.Dense(self.head_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Q = self.wq(inputs)\n",
    "        K = self.wk(inputs)\n",
    "        V = self.wv(inputs)\n",
    "\n",
    "        matmul_qk = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        depth = tf.cast(tf.shape(K)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "        output = tf.matmul(attention_weights, V)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b13227e-69a6-4f98-9882-1b3711c07a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class EntropyClassifier(tf.keras.Model):\n",
    "    def __init__(self, possible_labels):\n",
    "        super(EntropyClassifier, self).__init__()\n",
    "\n",
    "        self.possible_labels = possible_labels\n",
    "\n",
    "        self.dwt_input_layer = tf.keras.layers.Dense(10, activation='relu')\n",
    "        self.lvl0_input_layer = tf.keras.layers.Dense(17, activation='relu')\n",
    "        self.lvl1_input_layers = [tf.keras.layers.Conv2D(1, (2, 2), activation='relu') for _ in range(17)]\n",
    "        self.lvl2_input_layers = [tf.keras.layers.Conv2D(1, (2, 2), activation='relu') for _ in range(17)]\n",
    "        self.lvl3_input_layers = [tf.keras.layers.Conv2D(1, (2, 2), activation='relu') for _ in range(17)]\n",
    "\n",
    "        self.self_attention_heads = [SelfAttention(10, 1), SelfAttention(17, 1)] + [SelfAttention(4, 1) for _ in range(51)]\n",
    "\n",
    "        self.combination_layer = tf.keras.layers.Concatenate()\n",
    "\n",
    "        self.fc_layer = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(len(possible_labels), activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Method to forward propagate through the model\n",
    "        Args:\n",
    "        inputs (Tensor): Input tensor\n",
    "    \n",
    "        Returns:\n",
    "        Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        dwt_output, lvl0_output, lvl1_outputs, lvl2_outputs, lvl3_outputs = inputs\n",
    "    \n",
    "        # Passing the components through the respective layers\n",
    "        dwt_output = self.dwt_input_layer(dwt_output)\n",
    "        lvl0_output = self.lvl0_input_layer(lvl0_output)\n",
    "        lvl1_outputs = [layer(input) for layer, input in zip(self.lvl1_input_layers, lvl1_outputs)]\n",
    "        lvl2_outputs = [layer(input) for layer, input in zip(self.lvl2_input_layers, lvl2_outputs)]\n",
    "        lvl3_outputs = [layer(input) for layer, input in zip(self.lvl3_input_layers, lvl3_outputs)]\n",
    "        \n",
    "        # Expanding the dimensions of the outputs\n",
    "        lvl1_outputs = [tf.expand_dims(input, axis=0) for input in lvl1_outputs]\n",
    "        lvl2_outputs = [tf.expand_dims(input, axis=0) for input in lvl2_outputs]\n",
    "        lvl3_outputs = [tf.expand_dims(input, axis=0) for input in lvl3_outputs]\n",
    "    \n",
    "        attention_inputs = [dwt_output, lvl0_output] + [tf.keras.layers.Flatten()(output) for output in (lvl1_outputs + lvl2_outputs + lvl3_outputs)]\n",
    "        \n",
    "        attention_outputs = [layer(input) for layer, input in zip(self.self_attention_heads, attention_inputs)]\n",
    "        \n",
    "        # Calculate the common dimension size for concatenation\n",
    "        common_dimension_size = min([tf.shape(output)[-1] for output in attention_outputs])\n",
    "        \n",
    "        # Reshape tensors to have the same size in the last dimension\n",
    "        attention_outputs = [tf.slice(output, [0,0], [-1, common_dimension_size]) for output in attention_outputs]\n",
    "        \n",
    "        combined_output = self.combination_layer(attention_outputs)\n",
    "        \n",
    "        fc_output = self.fc_layer(combined_output)\n",
    "        output = self.output_layer(fc_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def train_model(self, dataset, epochs=100, batch_size=64, lr=0.01):\n",
    "        loss = None\n",
    "        formatted_dataset = format_dataset(dataset)\n",
    "\n",
    "        train_dataset = formatted_dataset.batch(batch_size)\n",
    "\n",
    "        criterion = CategoricalCrossentropy(from_logits=True)\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Starting epoch {epoch+1}/{epochs}')\n",
    "            for batch_idx, (data, target) in enumerate(train_dataset):\n",
    "                target = tf.convert_to_tensor([tf.one_hot(t, len(self.possible_labels)) for t in target], dtype=tf.float32)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    output = self(data, training=True)\n",
    "                    print(f\"Output shape: {output.shape}, Target shape: {target.shape}\")  # Added this line to debug\n",
    "                    loss = criterion(target, output)\n",
    "                gradients = tape.gradient(loss, self.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f'Batch {batch_idx}, Loss: {loss.numpy()}')\n",
    "\n",
    "            if loss is not None:\n",
    "                print(f'Epoch {epoch+1} completed, Loss: {loss.numpy()}')\n",
    "\n",
    "    def predict(self, image):\n",
    "        image = (image[0], image[1], image[2], image[3], image[4])\n",
    "        output_ = self(image, training=False)\n",
    "        probabilities = tf.nn.softmax(output_)\n",
    "        max_index = tf.argmax(probabilities)\n",
    "        label_prob_dict = {label: prob.numpy() for label, prob in zip(self.possible_labels, probabilities[0])}\n",
    "\n",
    "        return str(self.possible_labels[max_index.numpy()]), label_prob_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d63a42-3854-4365-a41b-87438ab6982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_entry(entry):\n",
    "    \"\"\"Process the entropy results to extract the levels.\"\"\"\n",
    "    label = entry['label']\n",
    "    machine_input = {0: [], 1: [], 2: [], 3: [], 'dwt': []}\n",
    "    for ent in entry['entropy_results']:\n",
    "    \n",
    "        if ent['method'] == 'dwt':\n",
    "            machine_input['dwt'] = tf.convert_to_tensor(ent['result'], dtype=tf.float32)\n",
    "        else:\n",
    "            for lvl, content in enumerate(ent['result']):\n",
    "                machine_input[lvl].append(tf.convert_to_tensor(content, dtype=tf.float32))\n",
    "    \n",
    "    machine_input[0] = tf.concat(machine_input[0], axis=-1)\n",
    "    machine_input[1] = tf.concat(machine_input[1], axis=-1)\n",
    "    machine_input[2] = tf.concat(machine_input[2], axis=-1)\n",
    "    machine_input[3] = tf.concat(machine_input[3], axis=-1)\n",
    "    machine_input['dwt'] = tf.reshape(machine_input['dwt'], [1, 1, 10])\n",
    "    \n",
    "    return {'input': machine_input, 'label': label}  \n",
    "\n",
    "\n",
    "def format_dataset(dataset):\n",
    "    \"\"\"Formats and shuffles the dataset for training\"\"\"\n",
    "    label_num = {'nature': 0, 'country': 1, 'city': 2}\n",
    "    formatted_dataset = []\n",
    "    for item in dataset:\n",
    "        machine_input = item['input']\n",
    "        label = label_num[item['label']]\n",
    "        formatted_dataset.append((\n",
    "            (\n",
    "                machine_input[0], \n",
    "                machine_input[1], \n",
    "                machine_input[2], \n",
    "                machine_input[3], \n",
    "                machine_input['dwt']\n",
    "            ), \n",
    "            label\n",
    "        ))\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        lambda: iter(formatted_dataset), \n",
    "        output_signature=(\n",
    "            (\n",
    "                tf.TensorSpec(shape=(1, 1, 17), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(2, 2, 17), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(4, 4, 17), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(8, 8, 17), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(1, 1, 10), dtype=tf.float32)\n",
    "            ),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "        )\n",
    "    ).shuffle(buffer_size=len(dataset))\n",
    "\n",
    "\n",
    "def process_json(path, test_part, parallel_jobs=4):\n",
    "    \"\"\"Process JSON data to extract dataset and features.\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    dataset = []\n",
    "\n",
    "    t = time.time()\n",
    "    n = len(metadata)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=parallel_jobs) as executor:\n",
    "        futures = [executor.submit(process_entry, entry) for entry in metadata]\n",
    "        for i, future in enumerate(as_completed(futures)):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                dataset.append(result)\n",
    "            print_progress_bar('Processed entry', i+1, n, t)\n",
    "\n",
    "    if isinstance(test_part, float):\n",
    "        i = int(test_part * len(dataset))\n",
    "    elif isinstance(test_part, str):\n",
    "        i = int(test_part)\n",
    "    else:\n",
    "        raise ValueError(\"Incompatible format for 'test_part'.\")\n",
    "\n",
    "    test_set = dataset[-i:]\n",
    "    dataset = dataset[:-i]\n",
    "\n",
    "    num_classes = len(all_labels)\n",
    "    dataset_length = len(dataset)\n",
    "\n",
    "    return dataset, test_set, num_classes, dataset_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2196569-713f-4310-bb1b-39221c6f2185",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# model evaluation functions\n",
    "def evaluate_model(model, test_set):\n",
    "    stats = {'test_samples': 0, 'right_predictions': 0}\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "    \n",
    "    for test in test_set:\n",
    "        stats['test_samples'] += 1\n",
    "        \n",
    "        image = test['image']\n",
    "        if device == \"GPU\":\n",
    "            image = tf.convert_to_tensor(image)\n",
    "        \n",
    "        predicted_label, label_probs = model.predict(image)\n",
    "        \n",
    "        y_true.append(test[\"label\"])\n",
    "        y_pred.append(predicted_label)\n",
    "        y_prob.append(label_probs[test[\"label\"][0]])\n",
    "        \n",
    "        if predicted_label == test[\"label\"]:\n",
    "            stats['right_predictions'] += 1\n",
    "            print(f'Predicted label: {predicted_label}.  Real label: {test[\"label\"]}. Prediction correct!')\n",
    "        else:\n",
    "            print(f'Predicted label: {predicted_label}.  Real label: {test[\"label\"]}. False prediction.')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    logloss = log_loss(y_true, np.array(y_prob))\n",
    "    \n",
    "    stats['success_rate'] = 100 * stats['right_predictions'] / stats['test_samples']\n",
    "    stats['confusion_matrix'] = conf_matrix\n",
    "    stats['precision'] = precision\n",
    "    stats['recall'] = recall\n",
    "    stats['log_loss'] = logloss\n",
    "    \n",
    "    print(f\"{stats['right_predictions']} samples out of {stats['test_samples']} were predicted correctly.\\n\"\n",
    "          f\"The model's success rate is: {stats['success_rate']}%\")\n",
    "    print(f\"Confusion Matrix: \\n{conf_matrix}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Log Loss: {logloss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7f62024-063c-4f89-b901-82f421741a94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed entry: ██████████████████████████████████████████████████ | Completed: 256/256 100.0% | Time elapsed: 00:01/00:01 | Time left: ~00:00Dataset processed.\n",
      "Total number of entries in the dataset: 244\n",
      "Total number of entries in the test set: 12\n",
      "Number of classes: 3\n",
      "Dataset length: 244\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataset, test_set, num_classes, dataset_length = process_json(path, test_part)\n",
    "print('Dataset processed.')\n",
    "print(f\"Total number of entries in the dataset: {dataset_length}\")\n",
    "print(f\"Total number of entries in the test set: {len(test_set)}\")    \n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Dataset length: {dataset_length}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "117ced19-69b5-4697-aa52-5db94b589f9f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'image'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m validate_dataset(dataset)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print(type(dataset[0]))\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(type(dataset[0]['image']))\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(key))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'image'"
     ]
    }
   ],
   "source": [
    "# data validity check\n",
    "# %whos\n",
    "validate_dataset(dataset)\n",
    "# print(type(dataset[0]))\n",
    "# print(type(dataset[0]['image']))\n",
    "for key in dataset[0]['image']:\n",
    "    print(type(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d58f1a48-0f23-46a5-be18-3e8533dc1e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created\n"
     ]
    }
   ],
   "source": [
    "# model creation\n",
    "file_name = f\"EntropyClassifier_e={epochs}_ds={dataset_length}.pth\"\n",
    "model = EntropyClassifier(all_labels)\n",
    "print('Model created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5db52a15-4d6d-479c-95f5-3d5509f9698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'entropy_classifier_1' (type EntropyClassifier).\n\nInput 0 of layer \"conv2d_51\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (4, 4, 17)\n\nCall arguments received by layer 'entropy_classifier_1' (type EntropyClassifier):\n  • inputs=('tf.Tensor(shape=(64, 1, 1, 17), dtype=float32)', 'tf.Tensor(shape=(64, 2, 2, 17), dtype=float32)', 'tf.Tensor(shape=(64, 4, 4, 17), dtype=float32)', 'tf.Tensor(shape=(64, 8, 8, 17), dtype=float32)', 'tf.Tensor(shape=(64, 1, 1, 10), dtype=float32)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel trained.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 76\u001b[0m, in \u001b[0;36mEntropyClassifier.train_model\u001b[1;34m(self, dataset, epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m     73\u001b[0m target \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor([tf\u001b[38;5;241m.\u001b[39mone_hot(t, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpossible_labels)) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m target], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 76\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Target shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Added this line to debug\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(target, output)\n",
      "File \u001b[1;32m~\\.conda\\envs\\entropy\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m, in \u001b[0;36mEntropyClassifier.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     33\u001b[0m dwt_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdwt_input_layer(dwt_output)\n\u001b[0;32m     34\u001b[0m lvl0_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlvl0_input_layer(lvl0_output)\n\u001b[1;32m---> 35\u001b[0m lvl1_outputs \u001b[38;5;241m=\u001b[39m [layer(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m layer, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlvl1_input_layers, lvl1_outputs)]\n\u001b[0;32m     36\u001b[0m lvl2_outputs \u001b[38;5;241m=\u001b[39m [layer(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m layer, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlvl2_input_layers, lvl2_outputs)]\n\u001b[0;32m     37\u001b[0m lvl3_outputs \u001b[38;5;241m=\u001b[39m [layer(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m layer, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlvl3_input_layers, lvl3_outputs)]\n",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     33\u001b[0m dwt_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdwt_input_layer(dwt_output)\n\u001b[0;32m     34\u001b[0m lvl0_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlvl0_input_layer(lvl0_output)\n\u001b[1;32m---> 35\u001b[0m lvl1_outputs \u001b[38;5;241m=\u001b[39m [\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m layer, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlvl1_input_layers, lvl1_outputs)]\n\u001b[0;32m     36\u001b[0m lvl2_outputs \u001b[38;5;241m=\u001b[39m [layer(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m layer, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlvl2_input_layers, lvl2_outputs)]\n\u001b[0;32m     37\u001b[0m lvl3_outputs \u001b[38;5;241m=\u001b[39m [layer(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m layer, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlvl3_input_layers, lvl3_outputs)]\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer 'entropy_classifier_1' (type EntropyClassifier).\n\nInput 0 of layer \"conv2d_51\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (4, 4, 17)\n\nCall arguments received by layer 'entropy_classifier_1' (type EntropyClassifier):\n  • inputs=('tf.Tensor(shape=(64, 1, 1, 17), dtype=float32)', 'tf.Tensor(shape=(64, 2, 2, 17), dtype=float32)', 'tf.Tensor(shape=(64, 4, 4, 17), dtype=float32)', 'tf.Tensor(shape=(64, 8, 8, 17), dtype=float32)', 'tf.Tensor(shape=(64, 1, 1, 10), dtype=float32)')"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.train_model(dataset, epochs=epochs, batch_size=batch_size, lr=learning_rate)\n",
    "print('Model trained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dc9932-29f0-4fbe-b8e2-cd18eb3e548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168957a-fdc3-4f50-94c5-795ae4533e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model.load_state_dict(torch.load(file_name, map_location=device))\n",
    "model.eval()\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d18773b-e2e5-4c83-bb4a-e031d84fe44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "evaluate_model(model, test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
