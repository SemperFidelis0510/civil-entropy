{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2173f0a-998d-4545-aa89-b45aae00dc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import data as tf_data\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, log_loss\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Union\n",
    "\n",
    "from utils import print_progress_bar\n",
    "print(\"Modules imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ee2ab7-dfd5-4b36-8281-db4eca3bb164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:\n",
      "/physical_device:CPU:0\n",
      "/physical_device:GPU:0\n",
      "The model will run on GPU: /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# variables\n",
    "all_labels = ['nature', 'country', 'city']\n",
    "path = \"../datasets/all_data/entropy_results_short.json\"\n",
    "parallel_jobs = 5\n",
    "\n",
    "# hyperparameters\n",
    "test_part = 0.05\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "\n",
    "# check gpu\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"Available devices:\")\n",
    "for device in devices:\n",
    "    print(device.name)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(f\"The model will run on GPU: {physical_devices[0].name}\")\n",
    "else:\n",
    "    print(\"No GPU found, the model will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3116f842-0887-4663-85b5-e7c9b5465da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heads\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(self.head_dim)\n",
    "        self.wk = tf.keras.layers.Dense(self.head_dim)\n",
    "        self.wv = tf.keras.layers.Dense(self.head_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Q = self.wq(inputs)\n",
    "        K = self.wk(inputs)\n",
    "        V = self.wv(inputs)\n",
    "\n",
    "        matmul_qk = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        depth = tf.cast(tf.shape(K)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "        output = tf.matmul(attention_weights, V)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfd21181-37c8-4055-bb6e-710aa6ea3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class EntropyClassifier(tf.keras.Model):\n",
    "    def __init__(self, possible_labels):\n",
    "        super(EntropyClassifier, self).__init__()\n",
    "    \n",
    "        self.possible_labels = possible_labels\n",
    "        self.debug = False\n",
    "    \n",
    "        self.dwt_input_layer = tf.keras.layers.Dense(10, activation='relu')\n",
    "        self.lvl0_input_layer = tf.keras.layers.Dense(17, activation='relu')\n",
    "        self.lvl1_input_layers = [tf.keras.layers.Conv2D(1, (2, 2), activation='relu') for _ in range(17)]\n",
    "        \n",
    "        self.lvl2_input_layers = [tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(1, (2, 2), activation='relu'), \n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        ]) for _ in range(17)]\n",
    "    \n",
    "        self.lvl3_input_layers = [tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(1, (2, 2), activation='relu'), \n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        ]) for _ in range(17)]\n",
    "\n",
    "        dwt_output_size = 10\n",
    "        lvl0_output_size = 17\n",
    "        lvl1_output_size = 17 \n",
    "        lvl2_output_size = 17  \n",
    "        lvl3_output_size = 153 \n",
    "\n",
    "        embed_size = dwt_output_size + lvl0_output_size + lvl1_output_size + lvl2_output_size + lvl3_output_size # = 214\n",
    "\n",
    "        heads = 1  # Choose based on your specific requirements or experimentation\n",
    "        assert embed_size % heads == 0, \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.self_attention = SelfAttention(embed_size, heads)\n",
    "\n",
    "        self.fc_layer = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(len(possible_labels), activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        lvl0_inputs, lvl1_inputs, lvl2_inputs, lvl3_inputs, dwt_inputs = inputs\n",
    "    \n",
    "        batch_size = tf.shape(dwt_inputs)[0]\n",
    "    \n",
    "        # Ensure inputs have a batch dimension\n",
    "        if len(lvl1_inputs.shape) == 3:\n",
    "            lvl1_inputs = tf.expand_dims(lvl1_inputs, axis=0)\n",
    "        if len(lvl2_inputs.shape) == 3:\n",
    "            lvl2_inputs = tf.expand_dims(lvl2_inputs, axis=0)\n",
    "        if len(lvl3_inputs.shape) == 3:\n",
    "            lvl3_inputs = tf.expand_dims(lvl3_inputs, axis=0)\n",
    "    \n",
    "        dwt_output = self.dwt_input_layer(dwt_inputs)\n",
    "        lvl0_output = self.lvl0_input_layer(lvl0_inputs)\n",
    "    \n",
    "        lvl1_output = tf.concat([self.lvl1_input_layers[i](lvl1_inputs[:, :, :, i:i+1]) for i in range(17)], axis=-1)\n",
    "        lvl2_output = tf.concat([self.lvl2_input_layers[i](lvl2_inputs[:, :, :, i:i+1]) for i in range(17)], axis=-1)\n",
    "        lvl3_output = tf.concat([self.lvl3_input_layers[i](lvl3_inputs[:, :, :, i:i+1]) for i in range(17)], axis=-1)\n",
    "    \n",
    "        concatenated_output = tf.concat([tf.reshape(dwt_output, [batch_size, -1]), \n",
    "                                         tf.reshape(lvl0_output, [batch_size, -1]), \n",
    "                                         tf.reshape(lvl1_output, [batch_size, -1]), \n",
    "                                         tf.reshape(lvl2_output, [batch_size, -1]), \n",
    "                                         tf.reshape(lvl3_output, [batch_size, -1])], axis=-1)\n",
    "        \n",
    "        attention_output = self.self_attention(concatenated_output)\n",
    "        fc_output = self.fc_layer(attention_output)\n",
    "        final_output = self.output_layer(fc_output)\n",
    "    \n",
    "        if self.debug:\n",
    "            print(dwt_output.shape)\n",
    "            print(lvl0_output.shape)\n",
    "            print(lvl1_output.shape)\n",
    "            print(lvl2_output.shape)\n",
    "            print(lvl3_output.shape)\n",
    "            print(concatenated_output.shape)\n",
    "            print(attention_output.shape)\n",
    "            print(fc_output.shape)\n",
    "            print(final_output.shape)\n",
    "    \n",
    "        return final_output\n",
    "\n",
    "    def train_model(self, dataset, epochs=100, batch_size=64, lr=0.01):\n",
    "        loss = None\n",
    "        formatted_dataset = format_dataset(dataset)\n",
    "\n",
    "        train_dataset = formatted_dataset.batch(batch_size)\n",
    "\n",
    "        criterion = CategoricalCrossentropy(from_logits=False)\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Starting epoch {epoch+1}/{epochs}')\n",
    "            for batch_idx, (data, target) in enumerate(train_dataset):\n",
    "                target = tf.convert_to_tensor([tf.one_hot(t, len(self.possible_labels)) for t in target], dtype=tf.float32)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    output = self(data, training=True)\n",
    "                    loss = criterion(target, output)\n",
    "                gradients = tape.gradient(loss, self.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "\n",
    "            if loss is not None:\n",
    "                print(f'Epoch {epoch+1} completed, Loss: {loss.numpy()}')\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        if len(inputs[0].shape) != 4 or inputs[0].shape[0] != 1:\n",
    "            raise ValueError(\"Input batch size should be 1\")\n",
    "    \n",
    "        output = self(inputs, training=False)\n",
    "        probabilities = tf.nn.softmax(output)\n",
    "        max_index = tf.argmax(probabilities)\n",
    "        label_prob_dict = {label: prob.numpy() for label, prob in zip(self.possible_labels, probabilities[0])}\n",
    "    \n",
    "        return str(self.possible_labels[max_index.numpy()]), label_prob_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9d27d0-c21e-46a9-9757-1bbffaf73605",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def process_entry(entry):\n",
    "    \"\"\"Process the entropy results to extract the levels.\"\"\"\n",
    "    label = entry['label']\n",
    "    machine_input = {0: [], 1: [], 2: [], 3: [], 'dwt': []}\n",
    "    for ent in entry['entropy_results']:\n",
    "    \n",
    "        if ent['method'] == 'dwt':\n",
    "            machine_input['dwt'] = tf.convert_to_tensor(ent['result'], dtype=tf.float32)\n",
    "        else:\n",
    "            for lvl, content in enumerate(ent['result']):\n",
    "                machine_input[lvl].append(tf.convert_to_tensor(content, dtype=tf.float32))\n",
    "    \n",
    "    machine_input[0] = tf.concat(machine_input[0], axis=-1)\n",
    "    machine_input[1] = tf.concat(machine_input[1], axis=-1)\n",
    "    machine_input[2] = tf.concat(machine_input[2], axis=-1)\n",
    "    machine_input[3] = tf.concat(machine_input[3], axis=-1)\n",
    "    machine_input['dwt'] = tf.reshape(machine_input['dwt'], [1, 1, 10])\n",
    "    \n",
    "    return {'input': machine_input, 'label': label}  \n",
    "\n",
    "\n",
    "def format_dataset(dataset):\n",
    "    \"\"\"Formats and shuffles the dataset for training\"\"\"\n",
    "    label_num = {'nature': 0, 'country': 1, 'city': 2}\n",
    "    formatted_dataset = []\n",
    "    for item in dataset:\n",
    "        machine_input = item['input']\n",
    "        label = label_num[item['label']]\n",
    "        formatted_dataset.append((\n",
    "            (\n",
    "                machine_input[0], \n",
    "                machine_input[1], \n",
    "                machine_input[2], \n",
    "                machine_input[3], \n",
    "                machine_input['dwt']\n",
    "            ), \n",
    "            label\n",
    "        ))\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        lambda: iter(formatted_dataset), \n",
    "        output_signature=(\n",
    "            (\n",
    "                tf.TensorSpec(shape=(1, 1, 17), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(2, 2, 17), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(4, 4, 17), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(8, 8, 17), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(1, 1, 10), dtype=tf.float32)\n",
    "            ),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "        )\n",
    "    ).shuffle(buffer_size=len(dataset))\n",
    "\n",
    "\n",
    "def process_json(path, test_part, parallel_jobs=4):\n",
    "    \"\"\"Process JSON data to extract dataset and features.\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    dataset = []\n",
    "\n",
    "    t = time.time()\n",
    "    n = len(metadata)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=parallel_jobs) as executor:\n",
    "        futures = [executor.submit(process_entry, entry) for entry in metadata]\n",
    "        for i, future in enumerate(as_completed(futures)):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                dataset.append(result)\n",
    "            print_progress_bar('Processed entry', i+1, n, t)\n",
    "\n",
    "    if isinstance(test_part, float):\n",
    "        i = int(test_part * len(dataset))\n",
    "    elif isinstance(test_part, str):\n",
    "        i = int(test_part)\n",
    "    else:\n",
    "        raise ValueError(\"Incompatible format for 'test_part'.\")\n",
    "\n",
    "    test_set = dataset[-i:]\n",
    "    dataset = dataset[:-i]\n",
    "\n",
    "    num_classes = len(all_labels)\n",
    "    dataset_length = len(dataset)\n",
    "\n",
    "    return dataset, test_set, num_classes, dataset_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e03c2cf-a8b4-4cbe-aaf1-85d07c1049a9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed entry: ██████████████████████████████████████████████████ | Completed: 256/256 100.0% | Time elapsed: 00:01/00:01 | Time left: ~= 00:00\n",
      "Dataset processed.\n",
      "Total number of entries in the dataset: 244\n",
      "Total number of entries in the test set: 12\n",
      "Number of classes: 3\n",
      "Dataset length: 244\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataset, test_set, num_classes, dataset_length = process_json(path, test_part)\n",
    "print('\\nDataset processed.')\n",
    "print(f\"Total number of entries in the dataset: {dataset_length}\")\n",
    "print(f\"Total number of entries in the test set: {len(test_set)}\")    \n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Dataset length: {dataset_length}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33bbba9a-3530-450a-9497-f99ceb6b22f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nature\n",
      "level: 0. shape: (1, 1, 17)\n",
      "level: 1. shape: (2, 2, 17)\n",
      "level: 2. shape: (4, 4, 17)\n",
      "level: 3. shape: (8, 8, 17)\n",
      "level: dwt. shape: (1, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "print(sample['label'])\n",
    "for inp, val in sample['input'].items():\n",
    "    print(f'level: {inp}. shape: {val.shape}')\n",
    "machine_inputs = tuple(sample['input'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4da293ed-4c21-4b66-9073-17f22d7463e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created\n"
     ]
    }
   ],
   "source": [
    "# model creation\n",
    "file_name = f\"EntropyClassifier_e={epochs}_ds={dataset_length}.pth\"\n",
    "model = EntropyClassifier(all_labels)\n",
    "print('Model created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90230190-9d12-4275-889c-0c6d08162a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3379935622215271, 0.49024030566215515, 0.17176614701747894]]\n"
     ]
    }
   ],
   "source": [
    "o = model.call(machine_inputs)\n",
    "print(o.numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d06c3e2-17bf-4ef8-81cf-7feaa7f803f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
